# training
epochs: 20
decoder_max_step: 3000

random_seed: False

batch_size: 64
learning_rate: 1.0e-3
weight_decay: 1.0e-6
grad_clip_thresh: 1.0

cache_dataset: False
use_cuda_if_available: True

balanced_sampling: False

# vocoder
vocoder_state_path: pretrained/hifigan-asc-v1/hifigan-asc.pth
vocoder_config_path: pretrained/hifigan-asc-v1/config.json

# diacritizers
shakkala_path: pretrained/diacritizers/shakkala_second_model6.pth
shakkelha_path: pretrained/diacritizers/shakkelha_rnn_3_big_20.pth


# restore_model: ''
restore_model: ./checkpoints/exp_tc2/states_9.pth
# restore_model: ./checkpoints/exp_tc2/states.pth

log_dir: logs/exp_tc2
checkpoint_dir: checkpoints/exp_tc2

# dataset
train_wavs_path: /content/drive/MyDrive/Tacotron2_Training/wavs
train_labels: /content/drive/MyDrive/Tacotron2_Training/metadata.csv

test_wavs_path: /content/drive/MyDrive/Tacotron2_Training/wavs
test_labels: /content/drive/MyDrive/Tacotron2_Training/metadata.csv

label_pattern: '"(?P<filename>.*)"|"(?P<phonemes>.*)"'
# label_pattern: (?P<arabic>.*)\|(?P<filestem>.*)

# optimizers
g_lr: 1.0e-3    # lr for AdamW optimizer (generator)
g_beta1: 0.9     # beta1 for AdamW optimizer (generator)
g_beta2: 0.999   # beta2 for AdamW optimizer (generator)

n_save_states_iter: 10
n_save_backup_iter: 1000